{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image composition assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# clone repository\n",
    "!git clone https://github.com/bcmi/Image-Composition-Assessment-with-SAMP.git\n",
    "#%cd Image-Composition-Assessment-with-SAMP/SAMPNet\n",
    "# download CADB data (~2GB), change the default dataset folder and gpu id in config.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision tensorboardX opencv-python scipy tqdm einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Launch the (modified) script to perform the assessment, specifying the storyboard directory and the maximum number of images to consider at a time\n",
    "!python \"path/to/test.py\" --custom_image_dir \"/path/to/storyboard/directory\" --batch_size 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts (w/o characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scene_prompts = [\n",
    " # List of prompts used to generated the storyboard, not containing the characters' lora name.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Function to extract the shot number from image filenames like \"shot_0.png\"\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'shot_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "\n",
    "# Function to load images from the folder and sort them based on the shot number in the image filname\n",
    "def load_images_from_folder(folder_path):\n",
    "    sorted_files = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.startswith('shot_') and f.lower().endswith(('.png', '.jpg', '.jpeg'))],\n",
    "        key=extract_number\n",
    "    )\n",
    "    \n",
    "    # Create full paths to the image files\n",
    "    sorted_image_paths = [os.path.join(folder_path, filename) for filename in sorted_files]\n",
    "    \n",
    "    return sorted_image_paths\n",
    "\n",
    "\n",
    "# Function to compute  CLIP score\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    # Convert PIL image to a NumPy array\n",
    "    images_np = np.array(images) / 255.0  # Normalize between 0 and 1\n",
    "    images_int = (images_np * 255).astype(\"uint8\")  # Scale and convert to uint8\n",
    "    \n",
    "    # Convert NumPy image to a tensor and rearrange the axes to match (batch_size, channels, height, width)\n",
    "    images_tensor = torch.from_numpy(images_int).permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score_result = clip_score_fn(images_tensor, prompts).detach()\n",
    "    return round(float(clip_score_result), 4)\n",
    "\n",
    "\n",
    "# Storyboard image folder to compute CLIP score on\n",
    "folder_path = \"/path/to/storyboard/directory\"   \n",
    "\n",
    "# Storyboard prompts \n",
    "prompts = [ ]\n",
    "\n",
    "# Load images from folder\n",
    "image_paths = load_images_from_folder(folder_path)\n",
    "\n",
    "# Ensure the number of images matches the number of prompts\n",
    "if len(image_paths) != len(prompts):\n",
    "    print(f\"Error: Number of images ({len(image_paths)}) does not match the number of prompts ({len(prompts)}).\")\n",
    "else:\n",
    "    # Initialize a variable to accumulate CLIP scores\n",
    "    total_clip_score = 0.0\n",
    "    # Iterate over each image-prompt pair and compute CLIP score\n",
    "    for i, (image_path, prompt) in enumerate(zip(image_paths, prompts)):\n",
    "        image = Image.open(image_path) # Open the image using PIL\n",
    "        clip_score = calculate_clip_score(image, prompt)\n",
    "        total_clip_score += clip_score  # Accumulate the score\n",
    "        print(f\"{clip_score}\")\n",
    "\n",
    "    # Calculate and print the average CLIP score\n",
    "    average_clip_score = total_clip_score / len(image_paths)\n",
    "    print(f\"\\nAverage CLIP score across all image-prompt pairs: {average_clip_score:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install git+https://github.com/facebookresearch/dinov2.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function to extract the numeric part from the filename for sorting\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'shot_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Function to load and sort images from a folder\n",
    "def load_images_from_folder(folder_path):\n",
    "    # Get the sorted list of files\n",
    "    sorted_files = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.endswith(('png', 'jpg', 'jpeg'))],\n",
    "        key=extract_number\n",
    "    )\n",
    "    # Full paths to the images\n",
    "    return [os.path.join(folder_path, filename) for filename in sorted_files]\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "\n",
    "# Folder of the storyboard for the fine-tuned model and the base model\n",
    "folder_path_1 = \"/path/to/finetuned/model/storyboard/direcory\"  \n",
    "folder_path_2 = \"/path/to/base/model/storyboard/direcory\"    \n",
    "\n",
    "images_folder_1 = load_images_from_folder(folder_path_1)\n",
    "images_folder_2 = load_images_from_folder(folder_path_2)\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(images_folder_1) != len(images_folder_2):\n",
    "    print(f\"Error: Folder 1 has {len(images_folder_1)} images, Folder 2 has {len(images_folder_2)} images.\")\n",
    "else:\n",
    "    print(f\"Processing {len(images_folder_1)} image pairs...\")\n",
    "\n",
    "    # Initialize cosine similarity function\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    # Accumulate similarity scores for average computation\n",
    "    total_similarity = 0.0\n",
    "\n",
    "    # Iterate through image pairs\n",
    "    for img1_path, img2_path in zip(images_folder_1, images_folder_2):\n",
    "        # Load and process image 1\n",
    "        image1 = Image.open(img1_path)\n",
    "        with torch.no_grad():\n",
    "            inputs1 = processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "            outputs1 = model(**inputs1)\n",
    "            image_features1 = outputs1.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Load and process image 2\n",
    "        image2 = Image.open(img2_path)\n",
    "        with torch.no_grad():\n",
    "            inputs2 = processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "            outputs2 = model(**inputs2)\n",
    "            image_features2 = outputs2.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = cos(image_features1[0], image_features2[0]).item()\n",
    "        dino_similarity_score = (similarity + 1) / 2  # Normalize to [0, 1]\n",
    "        \n",
    "        # Accumulate the similarity score\n",
    "        total_similarity += dino_similarity_score\n",
    "\n",
    "        # Print similarity for this image pair\n",
    "        print(f\"Similarity between {os.path.basename(img1_path)} and {os.path.basename(img2_path)}: {dino_similarity_score:.4f}\")\n",
    "    \n",
    "    # Calculate and print the average similarity\n",
    "    average_similarity = total_similarity / len(images_folder_1)\n",
    "    print(f\"\\nAverage Dino similarity across all image pairs: {average_similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5789785,
     "sourceId": 9511579,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5833532,
     "sourceId": 9570644,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5914565,
     "sourceId": 9684974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5793992,
     "sourceId": 9678196,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
