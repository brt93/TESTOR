{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbafd6c-5448-489e-8817-55cab3170c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "so far so good...\n"
     ]
    }
   ],
   "source": [
    "def numerizza(stringa):\n",
    "    #cstring=stringa.replace('(','').replace(')','').replace('[','').replace(\"'\",'').replace(']','')\n",
    "    tmp=stringa.split('), (')\n",
    "    #print(len(tmp))\n",
    "    final=[]\n",
    "    for i in range(len(tmp)):\n",
    "        \n",
    "        p_tok=tmp[i].replace('[','').replace(']','').replace('(','').replace(')','')\n",
    "        #d_tok=p_tok.replace(\"'\",'')\n",
    "        #print(p_tok)\n",
    "        tmp2=p_tok.split(',')\n",
    "        \n",
    "        shot_size=int(tmp2[0].strip())\n",
    "        #timedur=float(tmp2[1].strip())\n",
    "        final.append(shot_size)\n",
    "    \n",
    "    \n",
    "    return final\n",
    "\n",
    "text=[]\n",
    "oseq=[]\n",
    "cseq=[]\n",
    "gen=[]\n",
    "ids=[]\n",
    "durs=[]\n",
    "for i in range(len(data)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    text.append(data[\"text\"][i])\n",
    "    oseq.append(data[\"seq\"][i])\n",
    "    cseq.append(data[\"2d emb\"][i])\n",
    "    gen.append(data[\"genere\"][i])\n",
    "    ids.append(data[\"id\"][i])\n",
    "    durs.append(data[\"shot_durations\"][i])\n",
    "print(\"so far so good...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6b1e86-e9c4-4b9e-99bc-f23db71d8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "[3, 2, 2, 7, 2, 3, 2, 3, 1, 2, 2, 9, 2, 7, 2, 3, 2, 4, 3, 2, 4, 2, 5, 2, 4, 3, 3, 2, 5, 3, 2, 2, 2, 3, 4, 6, 2, 5, 4, 5, 4, 4, 3, 3, 4, 2, 5, 2, 3, 3, 3, 2, 4, 2, 3, 3, 3, 2, 3, 2, 9, 2, 3, 3, 2, 4, 3, 6, 3, 4, 2, 3, 4, 2, 3, 3, 3, 2, 2, 2, 3, 7, 2, 3, 2, 2, 9, 3]\n",
      "28735\n"
     ]
    }
   ],
   "source": [
    "test=[]\n",
    "n_d=[]\n",
    "n_id=[]\n",
    "n_text=[]\n",
    "for i in range(len(cseq)):\n",
    "    try:\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        tmp=numerizza(cseq[i])\n",
    "        n_d.append(durs[i])\n",
    "        n_text.append(text[i])\n",
    "        n_id.append(ids[i])\n",
    "        test.append(tmp)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "print(test[0])\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38d064a-86ef-4831-bf07-cc7a27504891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28735\n"
     ]
    }
   ],
   "source": [
    "print(len(n_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1309691-4404-47db-842c-1d84eacd1141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                      shot_sequence  \\\n",
      "0  Sv-BxH3SVS8  [3, 2, 2, 7, 2, 3, 2, 3, 1, 2, 2, 9, 2, 7, 2, ...   \n",
      "1  IsZdfna1LKA  [7, 9, 2, 2, 8, 2, 2, 9, 3, 3, 2, 7, 2, 8, 2, ...   \n",
      "2  tdzX5AKWiDw  [3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 2, 3, 5, 2, 3, ...   \n",
      "3  mycAsRhAr_M  [9, 3, 2, 3, 2, 3, 4, 6, 1, 4, 7, 4, 4, 5, 3, ...   \n",
      "4  WgXQnXPb-TA  [3, 3, 4, 3, 4, 3, 3, 2, 4, 3, 4, 4, 4, 4, 3, ...   \n",
      "\n",
      "                                                text  \\\n",
      "0  On the eve of the Rebellion, the players sing ...   \n",
      "1  Javert struggles with his conflicting morals a...   \n",
      "2  Violet Crawley passes on Downton's legacy to h...   \n",
      "3   Richard gives Thomas a memento before he leaves.   \n",
      "4  Tired of the pompous Royal Staff, the Downton ...   \n",
      "\n",
      "                                      shot_durations  \n",
      "0  [0.5, 2.8333333333333335, 2.8333333333333335, ...  \n",
      "1  [1.4583333333333333, 2.375, 2.8333333333333335...  \n",
      "2  [5.583333333333333, 5.041666666666667, 14.25, ...  \n",
      "3  [6.333333333333333, 3.0833333333333335, 1.625,...  \n",
      "4  [2.0833333333333335, 4.958333333333333, 2.6666...  \n",
      "   Unnamed: 0.1  Unnamed: 0           id  genere  \\\n",
      "0             0           0  Sv-BxH3SVS8       9   \n",
      "1             1           1  IsZdfna1LKA       2   \n",
      "2             2           2  tdzX5AKWiDw       2   \n",
      "3             3           3  mycAsRhAr_M       2   \n",
      "4             4           4  WgXQnXPb-TA       6   \n",
      "\n",
      "                                              2d emb  \\\n",
      "0  [(3, 2.0), (2, 3.0), (2, 4.0), (7, 4.0), (2, 3...   \n",
      "1  [(7, 3.0), (9, 3.0), (2, 8.0), (2, 8.0), (8, 5...   \n",
      "2  [(3, 8.0), (3, 50.0), (3, 53.0), (3, 5.0), (3,...   \n",
      "3  [(9, 3.0), (3, 2.0), (2, 3.0), (3, 5.0), (2, 2...   \n",
      "4  [(3, 3.0), (3, 3.0), (4, 8.0), (3, 2.0), (4, 3...   \n",
      "\n",
      "                                                 seq  \\\n",
      "0  [3, 3, 2, 2, 2, 2, 2, 2, 2, 7, 7, 7, 7, 2, 2, ...   \n",
      "1  [7, 7, 7, 9, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
      "2  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...   \n",
      "3  [9, 9, 9, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 2, ...   \n",
      "4  [3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, ...   \n",
      "\n",
      "                                                text  \\\n",
      "0  On the eve of the Rebellion, the players sing ...   \n",
      "1  Javert struggles with his conflicting morals a...   \n",
      "2  Violet Crawley passes on Downton's legacy to h...   \n",
      "3   Richard gives Thomas a memento before he leaves.   \n",
      "4  Tired of the pompous Royal Staff, the Downton ...   \n",
      "\n",
      "                                      shot_durations  \n",
      "0  [0.5, 2.8333333333333335, 2.8333333333333335, ...  \n",
      "1  [1.4583333333333333, 2.375, 2.8333333333333335...  \n",
      "2  [5.583333333333333, 5.041666666666667, 14.25, ...  \n",
      "3  [6.333333333333333, 3.0833333333333335, 1.625,...  \n",
      "4  [2.0833333333333335, 4.958333333333333, 2.6666...  \n"
     ]
    }
   ],
   "source": [
    "col=['id', '2d emb', \"text\"]\n",
    "\n",
    "new_data=pd.DataFrame()\n",
    "\n",
    "new_data['id']=n_id\n",
    "new_data['shot_sequence']=test\n",
    "new_data['text']=n_text\n",
    "new_data['shot_durations']=n_d\n",
    "print(new_data.head())\n",
    "#df.to_excel('output_file.xlsx', index=False)\n",
    "print(data.head())\n",
    "#new_data.to_excel('Sequenced_CMD.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13448ccd-2839-4d06-8678-c6b5a1e535f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_excel('Sequenced_CMD.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3e9b83-8687-42b8-8282-cd66052c94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"Sequenced_CMD.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da63729a-18dd-4446-b359-e72a77cb550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "so far so good...\n"
     ]
    }
   ],
   "source": [
    "text=[]\n",
    "\n",
    "cseq=[]\n",
    "\n",
    "ids=[]\n",
    "durs=[]\n",
    "for i in range(len(data)):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    text.append(data[\"text\"][i])\n",
    "    cseq.append(data[\"shot_sequence\"][i])\n",
    "    ids.append(data[\"id\"][i])\n",
    "    durs.append(data[\"shot_durations\"][i])\n",
    "print(\"so far so good...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10e5110d-36d1-4f8f-934d-2c093e5babc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b545369-360f-481e-906e-4e42ed51bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dfe7314e6148fa940177e5b44845ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8facca1d76f749c996048928e620a1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df64dcf0e134bd79f2d15dc0ac535ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d00cc18ee38475f849e3a8ba65b9d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e79bf3965640c1a7e30a55062877ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa1b3b4e54545c0a59ab278433d289a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'roberta-base'  # You can use different sizes of RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4f3bd89-c0d4-4b71-aff8-da6e5c274803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28735\n",
      "Sentence embeddings:\n",
      "28735\n",
      "28735\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "print(len(text))\n",
    "# Step 3: Pass the tokenized inputs through the RoBERTa model\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "\n",
    "# Step 4: Extract the embeddings\n",
    "last_hidden_states = output.last_hidden_state\n",
    "\n",
    "# Step 5: Compute mean pooling to get sentence embeddings\n",
    "sentence_embeddings = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(len(sentence_embeddings))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4441811-2294-445a-be10-bce37fb395d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28735\n",
      "28735\n",
      "28735\n",
      "28735\n",
      "text embedding extracted, now editing...\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_embeddings))\n",
    "print(len(ids))\n",
    "print(len(durs))\n",
    "print(len(cseq))\n",
    "print(\"text embedding extracted, now editing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00c408e8-1c08-446f-a170-4809d6b2149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "def dividi(lista):\n",
    "    tmp=lista.split(',')\n",
    "    final=[]\n",
    "    for i in range(len(tmp)):\n",
    "        dur_x=tmp[i].replace('[','').replace(']','').replace('(','').replace(')','')\n",
    "        \n",
    "        final.append(float(dur_x))\n",
    "        \n",
    "    return final\n",
    "        \n",
    "\n",
    "\n",
    "nmrz_seq=[]\n",
    "\n",
    "exc=0\n",
    "new_labs=[]\n",
    "new_ids=[]\n",
    "new_durs=[]\n",
    "new_text=[]\n",
    "for j in range(len(cseq)):\n",
    "    try:\n",
    "        \n",
    "        tmp_seq=dividi(cseq[j])\n",
    "        tmp_durs=dividi(durs[j])\n",
    "        new_durs.append(tmp_durs)\n",
    "        nmrz_seq.append(tmp_seq)\n",
    "        new_ids.append(ids[j])\n",
    "        new_text.append(sentence_embeddings[j])\n",
    "    except Exception as e:\n",
    "        \n",
    "        exc=exc+1\n",
    "        \n",
    "    \n",
    "print(exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d56e5ba3-9e61-4780-af62-bf99f58f4fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.0, 2.0, 7.0, 2.0, 3.0, 2.0, 3.0, 1.0, 2.0, 2.0, 9.0, 2.0, 7.0, 2.0, 3.0, 2.0, 4.0, 3.0, 2.0, 4.0, 2.0, 5.0, 2.0, 4.0, 3.0, 3.0, 2.0, 5.0, 3.0, 2.0, 2.0, 2.0, 3.0, 4.0, 6.0, 2.0, 5.0, 4.0, 5.0, 4.0, 4.0, 3.0, 3.0, 4.0, 2.0, 5.0, 2.0, 3.0, 3.0, 3.0, 2.0, 4.0, 2.0, 3.0, 3.0, 3.0, 2.0, 3.0, 2.0, 9.0, 2.0, 3.0, 3.0, 2.0, 4.0, 3.0, 6.0, 3.0, 4.0, 2.0, 3.0, 4.0, 2.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 7.0, 2.0, 3.0, 2.0, 2.0, 9.0, 3.0]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(nmrz_seq[0])\n",
    "print(nmrz_seq[0][0])\n",
    "#print(np.array(nmrz_seq[0]))\n",
    "#lala=np.array(nmrz_seq[0])\n",
    "#print(lala[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ef2104-fa8a-44ac-8df2-b3c24394737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28411\n"
     ]
    }
   ],
   "source": [
    "def create_onehot(element):\n",
    "    vocab_size = 10\n",
    "    fin=[]\n",
    "    \n",
    "    for j in range(10):\n",
    "        if int(element)==j:\n",
    "            tok=1\n",
    "        else:\n",
    "            tok=0\n",
    "        fin.append(tok)\n",
    "    return fin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_new_ids=[]\n",
    "simple_seq=[]\n",
    "\n",
    "onehot_seq=[]\n",
    "new_new_durs=[]\n",
    "text_emb=[]\n",
    "for i in range(len(nmrz_seq)):\n",
    "    new_s=[]\n",
    "    new_c=[]\n",
    "    new_o=[]\n",
    "    if len(nmrz_seq[i])<=100:\n",
    "        \n",
    "        for j in range(len(nmrz_seq[i])):\n",
    "            new_s.append(nmrz_seq[i][j])\n",
    "            \n",
    "            new_o.append(create_onehot(nmrz_seq[i][j]))\n",
    "        simple_seq.append(np.array(new_s))\n",
    "        \n",
    "        onehot_seq.append(np.array(new_o))\n",
    "        new_new_ids.append(new_ids[i])\n",
    "        new_new_durs.append(new_durs[i])\n",
    "        text_emb.append(new_text[i])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(len(simple_seq))\n",
    "#print(complex_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2d25797-15ae-4a9a-9616-90e692d32d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393/2312049769.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  tensor_sequences_v2 = torch.tensor(final_padded_sequences, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_complex_sequences(sequences, max_length):\n",
    "    #sequences=np.array(sequences).reshape(-1,)\n",
    "    feat=10\n",
    "    padded_sequences = np.zeros((len(sequences), max_length, feat))\n",
    "    for i in range(len(sequences)):\n",
    "        for j in range(max_length):\n",
    "            try:\n",
    "                padded_sequences[i][j]=sequences[i][j]\n",
    "            except Exception as e:\n",
    "                padded_sequences[i][j]=[1,0,0,0,0,0,0,0,0,0]\n",
    "                #padded_sequences[i][j]=[0.01,0,0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "num_sequences = len(simple_seq)\n",
    "\n",
    "# Pad sequences to a maximum length\n",
    "max_sequence_length = max(len(seq) for seq in onehot_seq)\n",
    "\n",
    "padded_sequences=pad_complex_sequences(onehot_seq, max_sequence_length)\n",
    "\n",
    "final_padded_sequences=[]\n",
    "for i in range(len(padded_sequences)):\n",
    "    try:\n",
    "        final_padded_sequences.append(padded_sequences[i])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "tensor_sequences_v2 = torch.tensor(final_padded_sequences, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e6c9927-412d-4516-bf47-e768d6dad542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28411, 1000])\n"
     ]
    }
   ],
   "source": [
    "flat=tensor_sequences_v2.view(len(tensor_sequences_v2), -1)\n",
    "\n",
    "print(flat.shape)\n",
    "#print(flat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b12544c-cb31-4900-83de-cf1ecaaaa7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "0\n",
      "Epoch [1/25], Train Loss: 0.0055, Test Loss: 0.0023, Train Accuracy: 0.9698, Test Accuracy: 0.9876\n",
      "1\n",
      "Epoch [2/25], Train Loss: 0.0019, Test Loss: 0.0009, Train Accuracy: 0.9904, Test Accuracy: 0.9958\n",
      "2\n",
      "Epoch [3/25], Train Loss: 0.0009, Test Loss: 0.0005, Train Accuracy: 0.9959, Test Accuracy: 0.9974\n",
      "3\n",
      "Epoch [4/25], Train Loss: 0.0006, Test Loss: 0.0004, Train Accuracy: 0.9976, Test Accuracy: 0.9980\n",
      "4\n",
      "Epoch [5/25], Train Loss: 0.0004, Test Loss: 0.0004, Train Accuracy: 0.9984, Test Accuracy: 0.9983\n",
      "5\n",
      "Epoch [6/25], Train Loss: 0.0003, Test Loss: 0.0003, Train Accuracy: 0.9989, Test Accuracy: 0.9985\n",
      "6\n",
      "Epoch [7/25], Train Loss: 0.0002, Test Loss: 0.0003, Train Accuracy: 0.9992, Test Accuracy: 0.9986\n",
      "7\n",
      "Epoch [8/25], Train Loss: 0.0002, Test Loss: 0.0003, Train Accuracy: 0.9994, Test Accuracy: 0.9987\n",
      "8\n",
      "Epoch [9/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9996, Test Accuracy: 0.9987\n",
      "9\n",
      "Epoch [10/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9997, Test Accuracy: 0.9987\n",
      "10\n",
      "Epoch [11/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9997, Test Accuracy: 0.9988\n",
      "11\n",
      "Epoch [12/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9997, Test Accuracy: 0.9988\n",
      "12\n",
      "Epoch [13/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9997, Test Accuracy: 0.9988\n",
      "13\n",
      "Epoch [14/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9988\n",
      "14\n",
      "Epoch [15/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9988\n",
      "15\n",
      "Epoch [16/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "16\n",
      "Epoch [17/25], Train Loss: 0.0001, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "17\n",
      "Epoch [18/25], Train Loss: 0.0000, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "18\n",
      "Epoch [19/25], Train Loss: 0.0000, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "19\n",
      "Epoch [20/25], Train Loss: 0.0000, Test Loss: 0.0003, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "20\n",
      "Epoch [21/25], Train Loss: 0.0000, Test Loss: 0.0004, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "21\n",
      "Epoch [22/25], Train Loss: 0.0000, Test Loss: 0.0004, Train Accuracy: 0.9999, Test Accuracy: 0.9989\n",
      "22\n",
      "Epoch [23/25], Train Loss: 0.0000, Test Loss: 0.0004, Train Accuracy: 0.9999, Test Accuracy: 0.9989\n",
      "23\n",
      "Epoch [24/25], Train Loss: 0.0000, Test Loss: 0.0004, Train Accuracy: 0.9999, Test Accuracy: 0.9989\n",
      "24\n",
      "Epoch [25/25], Train Loss: 0.0000, Test Loss: 0.0004, Train Accuracy: 0.9998, Test Accuracy: 0.9989\n",
      "Encoded shape: torch.Size([28411, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "#tentativo\n",
    "\n",
    "\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid activation for one-hot encoded vectors\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_samples = len(tensor_sequences_v2)\n",
    "n_timesteps = 100\n",
    "n_classes = 10  \n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(tensor_sequences_v2) * split_ratio)\n",
    "\n",
    "X_train=flat[:split_index]\n",
    "X_test=flat[split_index:]\n",
    "\n",
    "input_dim=1000\n",
    "# Parameters\n",
    "encoding_dim = 256\n",
    "enc_dim=200\n",
    "hidden_dim=2\n",
    "# Create the autoencoder model\n",
    "\n",
    "model = Autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for one-hot encoded vectors\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "#per normale autoencoder batch size 16\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "print(\"starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Training\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        predicted = outputs.round()\n",
    "        correct_train += (predicted == inputs).sum().item()\n",
    "        total_train += inputs.numel()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_accuracy = correct_train / total_train\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()  \n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            inputs = X_test[i:i+batch_size]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            \n",
    "            \n",
    "            predicted = outputs.round()\n",
    "            correct_test += (predicted == inputs).sum().item()\n",
    "            total_test += inputs.numel()\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_accuracy = correct_test / total_test\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss / len(X_train):.4f}, '\n",
    "          f'Test Loss: {test_loss / len(X_test):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, '\n",
    "          f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Encode the input data\n",
    "encoded_data = model.encoder(flat)\n",
    "\n",
    "print(\"Encoded shape:\", encoded_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e1d965-b460-440c-91ca-c6ae479814f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_Edit_autoencoder_trained_256.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3afa1-e3b0-4385-926c-9249253baab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c22573a-b830-4a2a-812d-5dc9de0a3314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28411, 100, 10)\n",
      "(28411, 1000)\n",
      "torch.Size([28411, 256])\n",
      "so far so good...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(padded_sequences.shape)\n",
    "to_emb=padded_sequences.reshape(-1,1000)\n",
    "print(to_emb.shape)\n",
    "  \n",
    "to_emb=torch.tensor(to_emb, dtype=torch.float)\n",
    "embedded_sequences=model.encoder(to_emb)\n",
    "\n",
    "print(embedded_sequences.shape)\n",
    "print(\"so far so good...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad5e6c12-a7ce-4b81-977d-7896afd61cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28411\n",
      "28411\n"
     ]
    }
   ],
   "source": [
    "print(len(new_new_ids))\n",
    "print(len(text_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65bd154f-dc6f-4fca-9f10-2ef8d8c1998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28411, 768])\n",
      "torch.Size([28411, 256])\n"
     ]
    }
   ],
   "source": [
    "tensor_t_emb = torch.stack(text_emb)\n",
    "\n",
    "\n",
    "print(tensor_t_emb.shape)\n",
    "print(embedded_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91abe419-0437-43be-ab33-b3a7a631dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       videoid  duration\n",
      "0  PK0i_dyx230        83\n",
      "1  cGTn7aRFttk       119\n",
      "2  Lf6fX3bsCxA        76\n",
      "3  zXBPTIjfZgA       132\n",
      "4  0JmETteiVzo       133\n"
     ]
    }
   ],
   "source": [
    "durations = pd.read_csv(\"durations.csv\")\n",
    "\n",
    "print(durations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19875270-bc2a-4402-a0fc-3640e098a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"starting...\")\n",
    "durs=[]\n",
    "edit_rate=[]\n",
    "for i in range(len(new_new_ids)):\n",
    "    if i%500==0:\n",
    "        print(i)\n",
    "    for j in range(len(durations[\"videoid\"])):\n",
    "        #if j%2500==0:\n",
    "        #    print(\"    \"+str(j))\n",
    "        if durations[\"videoid\"][j]==new_new_ids[i]:\n",
    "            token=durations[\"duration\"][j]\n",
    "            durs.append(token)\n",
    "            edit_rate.append(token/len(simple_seq[i]))\n",
    "\n",
    "print(len(durs))\n",
    "plt.hist(durs)\n",
    "plt.show()\n",
    "#print(durations[\"videoid\"][0])\n",
    "#print(type(durations[\"videoid\"][0]))\n",
    "print(new_new_ids[0])\n",
    "print(durations[\"videoid\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6b189-fa12-4c78-a525-7cf6094edb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edit_trends=[]\n",
    "cc0=0\n",
    "cc1=0\n",
    "cc2=0\n",
    "cc3=0\n",
    "cc4=0\n",
    "cspec=0\n",
    "rmax=0\n",
    "for i in range(len(complex_seq)):\n",
    "    \n",
    "    vec_sum=np.sum(complex_seq[i], axis=0)\n",
    "    tester=np.sum(vec_sum)\n",
    "    mm=sorted(vec_sum, reverse=True)\n",
    "    m1=mm[0]\n",
    "    m2=mm[1]\n",
    "    m3=mm[2]\n",
    "    r=tester-(m1+m2+m3)\n",
    "    \n",
    "    if r>m1:\n",
    "        edit_trends.append(0)\n",
    "        cc0=cc0+1\n",
    "    elif m1>=r and m2<r:\n",
    "        edit_trends.append(1)\n",
    "        cc1=cc1+1\n",
    "    elif m1>=r and m2>=r and m3<r:\n",
    "        edit_trends.append(2)\n",
    "        cc2=cc2+1\n",
    "    elif m1>=r and m2>=r and m3>=r and m1>=(m2+m3):\n",
    "        edit_trends.append(3)\n",
    "        cc3=cc3+1\n",
    "    elif m1>=r and m2>=r and m3>=r and m1<(m2+m3):\n",
    "        edit_trends.append(4)\n",
    "        cc4=cc4+1\n",
    "    else:\n",
    "        \n",
    "        cspec=cspec+1\n",
    "    \n",
    "        \n",
    "print(cc0)\n",
    "print(cc1)\n",
    "print(cc2)\n",
    "print(cc3)\n",
    "print(cc4)\n",
    "print(\"\\n\")\n",
    "print(cspec)\n",
    "print(rmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8c27b-19cf-4559-8b83-f78a481c4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_count=[]\n",
    "\n",
    "for i in range(len(simple_seq)):\n",
    "    shot_count.append(len(simple_seq[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5a7aeb1-1a76-4c63-8b9a-c5f88e429db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentence_embeddings = tensor_t_emb #torch.tensor([...])  # Your sentence embeddings\n",
    "\n",
    "torch.save(sentence_embeddings, \"final_sentence_embeddings.pt\")\n",
    "torch.save(embedded_sequences, \"final_sequence_embeddings.pt\")\n",
    "torch.save(shot_count, 'shot_count.pt')\n",
    "torch.save(edit_trends,'edit_trends_labels.pt')\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
